{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80be1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linux-pc/anaconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-10 17:05:24.961807: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-10 17:05:25.073391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746911125.121033   93377 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746911125.133139   93377 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746911125.236078   93377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746911125.236100   93377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746911125.236101   93377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746911125.236102   93377 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-10 17:05:25.246622: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/Event.csv', './data/EyeTrack.csv', './data/Motion.csv', './data/ECoG.csv']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob \n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "POC: Load Analyze-format MRI (.hdr/.img) and HDF5 data in Python\n",
    "Requires: nibabel, h5py, numpy, matplotlib, pandas\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import nibabel as nb\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "\n",
    "# Load data\n",
    "\n",
    "data_dir = \"./data/\"\n",
    "data_l = glob(data_dir + '*.csv')\n",
    "print(data_l)\n",
    "\n",
    "event_df = pd.read_csv(data_l[0])\n",
    "eye_track_df = pd.read_csv(data_l[1])\n",
    "motion_df = pd.read_csv(data_l[2])\n",
    "\n",
    "# Monkey K2 (Kuma)\n",
    "# Electrodes [52 -> 64] and [121 -> 128] (inclusive) are in the medial wall of the macaque brain.\n",
    "ecog_df = pd.read_csv(data_l[3])\n",
    "\n",
    "# Utility Helper Functions:\n",
    "\n",
    "def show_pipe_masks_on_image(raw_image, outputs):\n",
    "  plt.imshow(np.array(raw_image))\n",
    "  ax = plt.gca()\n",
    "  for mask in outputs[\"masks\"]:\n",
    "      show_mask(mask, ax=ax, random_color=True)\n",
    "  plt.axis(\"off\")\n",
    "  plt.show()\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3),\n",
    "                                np.array([0.6])],\n",
    "                               axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc00290",
   "metadata": {},
   "source": [
    "## Extracting centroids of ECoG array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a425774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"./imgs/K2.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e65ec3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b20664c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "segment_anything_pipeline = pipeline(\"mask-generation\", model=\"Zigeng/SlimSAM-uniform-77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f2c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = segment_anything_pipeline(raw_image, points_per_batch=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c477c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[\"masks\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fdc6684",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mimshow(np\u001b[38;5;241m.\u001b[39marray(raw_image))\n\u001b[1;32m      2\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mgca()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mask \u001b[38;5;129;01min\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(np.array(raw_image))\n",
    "ax = plt.gca()\n",
    "for mask in output[\"masks\"]:\n",
    "    color=np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    h,w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2488ef4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "show_pipe_masks_on_image(raw_image, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5510b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_centroids(image_path):\n",
    "#     # Read the image\n",
    "#     image = cv2.imread(image_path)\n",
    "#     gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "\n",
    "#     # Use HoughCircles to detect circles in the image\n",
    "#     circles = cv2.HoughCircles(\n",
    "#         gray, \n",
    "#         cv2.HOUGH_GRADIENT, dp=1.2, minDist=30,\n",
    "#         param1=50, param2=30, minRadius=10, maxRadius=100\n",
    "#     )\n",
    "\n",
    "#     # If circles are detected\n",
    "#     if circles is not None:\n",
    "#         circles = np.round(circles[0, :]).astype(\"int\")  # Convert to integer\n",
    "\n",
    "#         centroids = []\n",
    "#         for circle in circles:\n",
    "#             x, y, r = circle\n",
    "#             centroids.append((x, y))\n",
    "#             # Draw the circle and center on the image (optional for visualization)\n",
    "#             cv2.circle(image, (x, y), r, (0, 255, 0), 4)  # Green circle\n",
    "#             cv2.circle(image, (x, y), 2, (0, 0, 255), 3)  # Red center\n",
    "#         # Show the image with detected circles (optional)\n",
    "#         cv2.imshow(\"Detected Circles\", image)\n",
    "#         cv2.waitKey(0)\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "#         return centroids\n",
    "#     else:\n",
    "#         print(\"No circles were detected.\")\n",
    "#         return []\n",
    "\n",
    "# # Example usage\n",
    "# image_path = \"./imgs/K2.png\"\n",
    "# centroids = extract_centroids(image_path)\n",
    "# print(\"Centroid Coordinates:\", centroids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55875329",
   "metadata": {},
   "source": [
    "# MRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaba956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Paths (edit these to your local files)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "analyze_hdr = \"./data/mri/K2_t1.hdr\"   # header for Analyze 7.5\n",
    "# analyze_img = \"./data/mri/K2_t1.img\"   # binary data file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32fa0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Load Analyze-format volume with NiBabel\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# NiBabel will read the header and image automatically from the .hdr/.img pair.\n",
    "mri_img = nb.load(analyze_hdr)             # you can equally pass analyze_img here\n",
    "mri_data = mri_img.get_fdata(dtype=np.float32)\n",
    "print(f\"Analyze volume shape: {mri_data.shape}\")\n",
    "print(f\"Affine transform (vox→mm):\\n{mri_img.affine}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a middle axial slice\n",
    "mid_z = mri_data.shape[2] // 2\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(mri_data[:, :, mid_z].T, origin=\"lower\", cmap=\"gray\")\n",
    "plt.title(\"Middle axial slice of Analyze volume\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96862ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Summarize intensity distribution with pandas\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "flat = mri_data.flatten()\n",
    "stats = {\n",
    "    \"minimum\": flat.min(),\n",
    "    \"maximum\": flat.max(),\n",
    "    \"mean\":    flat.mean(),\n",
    "    \"stddev\":  flat.std()\n",
    "}\n",
    "df = pd.DataFrame(stats, index=[\"AnalyzeVolume\"])\n",
    "print(\"\\nIntensity summary:\")\n",
    "print(df.to_string())\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# End of proof-of-concept\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24c684",
   "metadata": {},
   "source": [
    "# Identifying where the centroids of the ecog array are placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c430bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1. Paths: replace these with your actual files\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# mri_nii   = \"./path/to/preop_T1.nii.gz\"       # your subject’s structural MRI\n",
    "ct_nii    = \"./imgs/K2.png\" # \"/path/to/postop_CT_with_ECoG.nii.gz\"\n",
    "atlas_nii = \"./data/atlas/D99_v2.0_dist/D99_atlas_v2.0.nii.gz\"  # e.g. CHARM or D99\n",
    "labels_txt = \"./data/atlas/D99_v2.0_dist/D99_v2.0_labels_semicolon.txt\"\n",
    "labels_csv= \"./data/atlas/D99_v2.0_dist/D99_v2.0_labels_semicolon.csv\"           # CSV: {index,label_name}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61d62cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting text file to csv\n",
    "# df = pd.read_csv(labels_txt, sep=\";\")\n",
    "# df.to_csv(labels_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 2. Load images\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "mri_img   = nb.load(analyze_hdr)\n",
    "# ct_img    = nb.load(ct_nii)\n",
    "atlas_img = nb.load(atlas_nii)\n",
    "\n",
    "mri_data   = mri_img.get_fdata()\n",
    "# ct_data    = ct_img.get_fdata()\n",
    "atlas_data = atlas_img.get_fdata().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aec7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3. Rigid-coregistration: estimate affine from CT → MRI\n",
    "#    (In practice you’d do this once in ANTs or FSL and save the transform;\n",
    "#     here we show a simple mutual-information based init from nibabel’s example)\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "from nibabel.processing import resample_from_to\n",
    "# Resample CT into MRI space (nearest for electrodes)\n",
    "ct2mri = resample_from_to(ct_img, mri_img, order=0)  # order=0 preserves labels\n",
    "ct2mri_data = ct2mri.get_fdata()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 4. Segment electrodes in CT (thresholding bright contacts),\n",
    "#    find their centroids in MRI space\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# set a threshold high enough to isolate electrodes\n",
    "thr = np.percentile(ct2mri_data[ct2mri_data>0], 99.5)\n",
    "bw  = ct2mri_data > thr\n",
    "\n",
    "# label connected components and compute centroids\n",
    "labeled, n_labels = ndimage.label(bw)\n",
    "objects = ndimage.find_objects(labeled)\n",
    "centroids = []\n",
    "for label_idx in range(1, n_labels+1):\n",
    "    coords = np.argwhere(labeled == label_idx)\n",
    "    if coords.size == 0:\n",
    "        continue\n",
    "    # mean in voxel space\n",
    "    centroid_vox = coords.mean(axis=0)\n",
    "    # convert to world\n",
    "    centroid_mri = nb.affines.apply_affine(mri_img.affine, centroid_vox[::-1])\n",
    "    centroids.append(centroid_mri)\n",
    "centroids = np.array(centroids)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 5. Map each centroid into atlas indices and lookup label\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Invert atlas affine\n",
    "inv_atlas_affine = np.linalg.inv(atlas_img.affine)\n",
    "labels = []\n",
    "for xyz in centroids:\n",
    "    vox = nb.affines.apply_affine(inv_atlas_affine, xyz)[::-1]\n",
    "    vox = np.round(vox).astype(int)\n",
    "    # guard against out-of-bounds:\n",
    "    if np.any(vox < 0) or vox[0]>=atlas_data.shape[0] or vox[1]>=atlas_data.shape[1] or vox[2]>=atlas_data.shape[2]:\n",
    "        labels.append(\"out_of_brain\")\n",
    "    else:\n",
    "        lbl_idx = int(atlas_data[tuple(vox)])\n",
    "        labels.append(lbl_idx)\n",
    "\n",
    "# load your CSV of index→region\n",
    "import csv\n",
    "idx2name = {}\n",
    "with open(labels_csv) as f:\n",
    "    reader = csv.reader(f)\n",
    "    for idx,name in reader:\n",
    "        idx2name[int(idx)] = name\n",
    "\n",
    "# translate indices to names\n",
    "region_names = [idx2name.get(i, \"unknown\") for i in labels]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 6. Print out electrode → region table\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    \"electrode_id\": np.arange(len(centroids)) + 1,\n",
    "    \"x_mm\": np.round(centroids[:,0],1),\n",
    "    \"y_mm\": np.round(centroids[:,1],1),\n",
    "    \"z_mm\": np.round(centroids[:,2],1),\n",
    "    \"atlas_index\": labels,\n",
    "    \"region\": region_names\n",
    "})\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 7. (Optional) Quick coronal slice with electrodes overlaid\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "slice_z = int(mri_data.shape[2]//2)\n",
    "plt.imshow(mri_data[:,:,slice_z].T, cmap=\"gray\", origin=\"lower\")\n",
    "pts = []\n",
    "for (x,y,z), name in zip(centroids, region_names):\n",
    "    if abs(z - nb.affines.apply_affine(mri_img.affine,[0,0,slice_z])[2])<2:\n",
    "        pts.append((x, y))\n",
    "xs, ys = zip(*pts)\n",
    "plt.scatter(xs, ys, s=30, c=\"red\")\n",
    "plt.title(\"Coronal slice with detected ECoG centroids\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
